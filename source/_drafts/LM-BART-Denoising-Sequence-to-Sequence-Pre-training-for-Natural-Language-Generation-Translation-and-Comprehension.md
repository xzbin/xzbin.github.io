---
title: >-
  BART: Denoising Sequence-to-Sequence Pre-training for Natural Language
  Generation, Translation, and Comprehension
tags: 
    - [BART]
categories: 
  - [论文阅读]
  - [LM]
---

* 原始论文: [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
* 非官方代码：[bart](https://huggingface.co/docs/transformers/model_doc/bart)

# 文章idea
# 文章主旨
# 模型架构
# 模型实验